{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'valid']\n"
     ]
    }
   ],
   "source": [
    "# FROM 2012 to 2023, IMAGE Processing is Lagging behind NLP because of Image Size, Volume etc\n",
    "# NLP because it's smaller, it potentially is a easier thing to practically do. But until 2019, even google didn't follow through\n",
    "\n",
    "# ! KEY CODE\n",
    "import datasets as huggingface_datasets\n",
    "import torchvision\n",
    "\n",
    "dataset = huggingface_datasets.load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "print(huggingface_datasets.get_dataset_split_names(\"zh-plus/tiny-imagenet\"))\n",
    "training_dataset, validation_dataset = dataset['train'], dataset['valid']\n",
    "\n",
    "\n",
    "transformations_list = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "def transform_datasets(examples):\n",
    "    examples[\"converted_tensors\"] = []\n",
    "    \n",
    "    for image in examples['image']:\n",
    "        transformed_image = transformations_list(image)\n",
    "        examples['converted_tensors'].append(transformed_image)\n",
    "    \n",
    "    return examples\n",
    " \n",
    "training_dataset.set_transform(transform_datasets)\n",
    "validation_dataset.set_transform(transform_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0,\n",
       " 'converted_tensors': tensor([[[1.0000, 1.0000, 0.9647,  ..., 0.8157, 0.8118, 0.4902],\n",
       "          [1.0000, 1.0000, 0.9647,  ..., 0.7961, 0.7843, 0.4745],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8118, 0.7843, 0.4667],\n",
       "          ...,\n",
       "          [0.3686, 0.3647, 0.3647,  ..., 0.3333, 0.3059, 0.3686],\n",
       "          [0.3490, 0.3451, 0.3412,  ..., 0.3373, 0.3020, 0.3412],\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3765, 0.3020, 0.2824]],\n",
       " \n",
       "         [[0.5373, 0.5451, 0.5804,  ..., 0.9294, 0.9451, 0.6431],\n",
       "          [0.4863, 0.5020, 0.5373,  ..., 0.9098, 0.9216, 0.6275],\n",
       "          [0.4863, 0.4980, 0.5255,  ..., 0.9137, 0.9216, 0.6118],\n",
       "          ...,\n",
       "          [0.4784, 0.4667, 0.4549,  ..., 0.2902, 0.2627, 0.3255],\n",
       "          [0.4627, 0.4549, 0.4431,  ..., 0.2941, 0.2588, 0.2980],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.3333, 0.2588, 0.2392]],\n",
       " \n",
       "         [[0.7529, 0.7529, 0.7725,  ..., 0.9216, 0.9412, 0.6314],\n",
       "          [0.7412, 0.7412, 0.7608,  ..., 0.9020, 0.9059, 0.6157],\n",
       "          [0.7882, 0.7882, 0.7961,  ..., 0.9020, 0.8980, 0.5843],\n",
       "          ...,\n",
       "          [0.2784, 0.2784, 0.2745,  ..., 0.2196, 0.1922, 0.2549],\n",
       "          [0.2510, 0.2549, 0.2588,  ..., 0.2235, 0.1882, 0.2275],\n",
       "          [0.2588, 0.2667, 0.2745,  ..., 0.2627, 0.1882, 0.1686]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Kernel Shape              Param #                   Trainable                 Param %\n",
      "==============================================================================================================================================================================================\n",
      "Sequential                               [1, 3, 64, 64]            [1, 10]                   --                        --                        True                           --\n",
      "├─Conv2d: 1-1                            [1, 3, 64, 64]            [1, 50, 1, 1]             [64, 64]                  614,450                   True                       99.92%\n",
      "│    └─weight                                                                                [3, 50, 64, 64]           ├─614,400\n",
      "│    └─bias                                                                                  [50]                      └─50\n",
      "├─ReLU: 1-2                              [1, 50, 1, 1]             [1, 50, 1, 1]             --                        --                        --                             --\n",
      "├─Flatten: 1-3                           [1, 50, 1, 1]             [1, 50]                   --                        --                        --                             --\n",
      "├─Linear: 1-4                            [1, 50]                   [1, 10]                   --                        510                       True                        0.08%\n",
      "│    └─weight                                                                                [50, 10]                  ├─500\n",
      "│    └─bias                                                                                  [10]                      └─10\n",
      "==============================================================================================================================================================================================\n",
      "Total params: 614,960\n",
      "Trainable params: 614,960\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.61\n",
      "==============================================================================================================================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 2.46\n",
      "Estimated Total Size (MB): 2.51\n",
      "==============================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinkya/opt/anaconda3/lib/python3.9/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(out_channels = 50001, in_channels = 3, kernel_size = (64, 64) ),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Flatten(start_dim=1),\n",
    "    \n",
    "    nn.Linear(out_features = 10, in_features = 50)\n",
    ")\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(simple_model, input_size=(1,3,64,64), \n",
    "        verbose=2, col_names = [\"input_size\", \"output_size\",\"kernel_size\", \"num_params\",\"trainable\", \"params_percent\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(out_channels = 5, in_channels = 3, kernel_size = (64, 64) )\n",
    "        self.layer2 = nn.Linear(out_features = 8, in_features = 5)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        layer1_output = self.layer1(input_batch)\n",
    "        # layer1_output = torch.dot(input_batch, self.layer1.weight)\n",
    "\n",
    "        activation_function_output = nn.functional.relu(layer1_output)\n",
    "\n",
    "        layer2_output = self.layer2(activation_function_output)\n",
    "        activation_function_output = nn.functional.relu(layer2_output)\n",
    "\n",
    "        return activation_function_output\n",
    "\n",
    "model = Custom_Model()\n",
    "\n",
    "single_random_batch = torch.randn(1,3,64,64)\n",
    "model(single_random_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.ophthalmologytraining.com/images/Stills/Visual%20Cortex.jpg)\n",
    "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9b02ffb4-e1c2-46ba-9170-397bd0ececae_1280x451.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "# Data: CAT OR A DOG DETECTOR\n",
    "# Data: MICRO FRACTURE DETECTOR FROM XRAY\n",
    "# Data: Satellite Data. We want to extract population & farming stats. \n",
    "# TODO: MODEL \n",
    "feature_extractor = nn.Sequential(\n",
    "    nn.Conv2d( out_channels = 10, in_channels = 3, kernel_size = (3,3)), # visual cortex, v1: Lines & Edges\n",
    "    nn.ReLU(),\n",
    "  \n",
    "    nn.Conv2d(out_channels = 10, in_channels = 10, kernel_size = (3,3)), # visual cortex, v2: Textures & Patterns\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv2d(out_channels = 10, in_channels = 10, kernel_size = (3,3)), \n",
    "    # visual cortex, v3: Parts of Objects. (Extracting all parts of object for all classes)\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv2d(out_channels = 2, in_channels = 10, kernel_size = (3,3)), # visual cortex, v4: Objects\n",
    "    # all parts belonging to the same object, are combined and we have a single neuron, which detects, that specfic object. \n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "decision_maker = nn.Sequential(\n",
    "  nn.Linear(out_features = 50, in_features = 100*7*7 ),\n",
    "  nn.Linear(out_features = 10, in_features = 50)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "  feature_extractor,\n",
    "  decision_maker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = nn.Sequential(\n",
    "    nn.Conv2d( out_channels = 10, in_channels = 3, kernel_size = (3,3)), # visual cortex, v1: Lines & Edges\n",
    "    nn.ReLU(),\n",
    "  \n",
    "    nn.Conv2d(out_channels = 10, in_channels = 10, kernel_size = (3,3)), # visual cortex, v2: Textures & Patterns\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv2d(out_channels = 10, in_channels = 10, kernel_size = (3,3)), \n",
    "    # visual cortex, v3: Parts of Objects. (Extracting all parts of object for all classes)\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv2d(out_channels = 2, in_channels = 10, kernel_size = (3,3)), # visual cortex, v4: Objects \n",
    "    # all parts belonging to the same object, are combined and we have a single neuron, which detects, that specfic object. \n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/Users/ajinkya/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/Users/ajinkya/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ajinkya/Documents/Visual Studio Code/0_PROJECTS/1 - Vision 101 Workshop/final_slides/Week 3 - Imagenet.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m6\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m BASELINE_MODEL()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ajinkya/Documents/Visual Studio Code/0_PROJECTS/1 - Vision 101 Workshop/final_slides/Week 3 - Imagenet.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_batch):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     v1_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mv1(input_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     v1_activation_output \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(v1_output)\n",
      "\u001b[1;32m/Users/ajinkya/Documents/Visual Studio Code/0_PROJECTS/1 - Vision 101 Workshop/final_slides/Week 3 - Imagenet.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_batch):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     v1_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mv1(input_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ajinkya/Documents/Visual%20Studio%20Code/0_PROJECTS/1%20-%20Vision%20101%20Workshop/final_slides/Week%203%20-%20Imagenet.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     v1_activation_output \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(v1_output)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1087\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1078\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "class BASELINE_MODEL(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.v1 = nn.Conv2d( out_channels = 10, in_channels = 3, kernel_size = (4,4)) # visual cortex, v1: Lines & Edges\n",
    "\n",
    "    # FORWARD. DATA BATCH, is passed left to right, through the layers, one by one. \n",
    "    def forward(self, input_batch):\n",
    "        v1_output = self.v1(input_batch)\n",
    "        # v1_ouput -> feature_map\n",
    "        shape = v1_output.shape\n",
    "        v1_activation_output = nn.functional.relu(v1_output)\n",
    "\n",
    "x = torch.randn(1,3,6,6)\n",
    "\n",
    "model = BASELINE_MODEL()\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,16,16)\n",
    "layer = nn.Conv2d( out_channels = 10, in_channels = 3, kernel_size = (9,9), stride = 4) # kernel = (3,8,8) image = 3,16,16\n",
    "# output = 10, 2, 2\n",
    "\n",
    "layer(x).shape\n",
    "\n",
    "l2 = nn.Conv2d( out_channels = 20, in_channels = 10, kernel_size = (8,8))\n",
    "l2(layer(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRIVER, MECHANIC, AUTOMOBILE ENGINEER\n",
    "# ! MATHS GRADIENT DESCENT, MATHS LINEAR ALGEBRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
