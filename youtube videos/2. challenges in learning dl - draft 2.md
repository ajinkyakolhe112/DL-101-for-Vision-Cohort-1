
## Agenda
- Coding != Deep Learning
- Neural Networks as Programming or Maths
- Two Popular Approaches of Learning
- Learning Resources & Learning Plan
- Jargons & Jargons
- Overview of Deep Learning
---
### Coding != Deep Learning
- My overconfidence. I am good enough Programmer
- Machine Learning Trainer
- In college I had learnt Deep Learning. 
- Overconfidence and illusion of expertise. Neglecting science & experimentation aspect. neglecting maths & detailed understanding aspect. 
- People don't do Science in Data Science. They don't do Maths in Data Science. They just do Coding in Science.

--- 

### 1. Neural Network -> How to Look at it
- Automobile Engineer   vs  Mechanic     vs  Driver
- Maths                 vs  Coding       vs  Using the Intelligence as Blackbox
![test](https://padaseva.in/wp-content/uploads/2019/12/elephant-blind-men.jpg)
- Partial Picture, not complete End to End Picture
---
### 2. Two Popular Approaches: Andrew Ng or Jeremy Howard & My Struggles with them
- Either too detailed too quickly. Automobile Engineer in Andrew Ng
- Or top down but code too complex for beginning, not abstracted. Mechanic in Jeremy Howard
- No Middle Way. [NN from scratch by sentdex] a better start. [NN - 3Blue1Brown]
- Had to build a Hybrid Model
---
### 3. A mix of different courses + blogs + etc
- Choosing the right kind of courses was difficult. 
- Another difficult. No Standardization. variable names, function names or in pytorch the names are simple. but code isn't written in standard way
- Code naming is succint. But in the beginning, code should be written to be read. Not just for compiler.
- `y_preds` vs `y_predicted_logits` vs `y_predicted_pre_activation`
- Python language -> one way to do one thing vs Perl language -> many ways to do one thing. With pytorch, we get lost between different ways of writing the model
- Before focusing on essential concepts
---
### 4. Jargons, Jargons & Jargons
Gradient Descent, Stochastic Gradient Descent, Relu. MNIST, CIFAR, Imagenet, alexnet, vgg19, resnet
---

### 5. Resources - Need for Landscape
- A detailed landscape. Landscape not as simple venn diagram. 
- But checklist of skills. like video game skilltree
- Landscape of Libraries too: Do both. 
  - Low Level/Programming: Write from scratch
  - Low Level: Pytorch
  - High Level: Fast ai / Keras / Huggingface may be? . Write your own code snippets for things.
  - Custom Functionality: External Libraries ready made, or when you might need to write it yourself, in pytorch

----


- 3. Automobile Engineer -> Everyone talks about Neuron & Activation function. No matter what its same. But it doesn't mean we can fix car or drive., Mechanic, Car Driver
- (ML trainer, DE trainer, but not transferable in Deep Learning) 5. Codes are not written to be read or understood. Even though Pytorch is pythonic, and Python is Pythonic, it doesn't mean code is pythonic. Parameter names
(started learning. and got stuck) 1. Two paths. Andrew Ng + maths heavy. Top Down Jeremy Howard. Hit wall in both
- (same thing repeated in different ways increasing confusion) 4. No Standardization. Perl vs Python Philosophy... 
- 7. Jargons, Jargons. Gradient Descent, Stochastic Gradient Descent, Relu. MNIST, CIFAR, Imagenet, alexnet, vgg19, resnet. Model Parameters vs Function Parameters
Too many complex name terms, not easy to understand explainations


1. DL is blind, error minimization. It doesn't actually understand anything. 
2. DL can't be understood completely at least in our lifetime. Even though its progressing very fast, we are just building bigger models. Law Scaling or Emergence of LLM intelligence

3. Too many new papers coming constantly. I was overwhelmed. I could only understand a bit of things, and missed most things.

4.  DL is Science & Art both.. It is tough for both programmers and non programmers. 
for programmers - coding part is easy. 
for non programmers - DL concepts & programming both is hard

Through this I figured out, the need of landscape. which will help in learning. 